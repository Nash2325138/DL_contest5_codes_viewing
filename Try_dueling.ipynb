{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "(288, 512, 3)\n",
      "(288, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADfCAYAAAD4Bhh5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAEQZJREFUeJzt3H+sZGV9x/H3p6jQ+qOA6GZd1oJ4mwYTXc0G12AM/lqRmOJPYEOUEJL1D0wxIWmAJkWT0tikSkvSkq4BxYQuYMBACCq4khD/4MeCCAuIuyiE3S5sRURTE1vg2z/m3GVY7t45M3dmZ8/c9yuZzDnPOTPzzLOZz/3uM2eeVBWSpNn1J9PugCRpsgx6SZpxBr0kzTiDXpJmnEEvSTPOoJekGTexoE9ycpJHk+xIcsGkXkeStLhM4jr6JIcAvwA+CuwE7gE2VNXDY38xSdKiJlXRnwDsqKpfVtX/AtcAp07otSRJi3jVhJ53FfBk3/5O4L37OzmJP8+VtKA3vOEIAN46dwR7nnv2FccPPezl+8899yxvPbT3GA47YtLdm6pt237566p606DzJhX0AyXZCGyc1utL6oYTT/wIAJfd8jku+/53X3Zsbu6V53//+9/lsrd/pDnhc5Pu3lTNzZ32RJvzJhX0u4DVfftHN217VdUmYBNY0UvSJE1qjv4eYC7JsUleA5wB3DSh15K0DC1UzWthE6noq+r5JF8CfggcAlxZVQ9N4rUkLS8G/PAmNkdfVbcAt0zq+SUtP4NCfvv2A9OPrvGXsZI046Z21Y0kjYuV/OKs6CV1miE/mEEvqbMM+XYMekmdZMi3Z9BL0ozzy1hJnWIlPzwrekmdMSjk/THVwqzoJXWeAb84K3pJmnEGvaROs5ofzKCX1FmGfDsGvaROMuTbM+glacZ51Y2kTrGSH54VvaTOcD360VjRS+o8A35xVvSSNOMMekmdZjU/mEEvqbMM+XYMekmdZMi3Z9BL0ozzqhtJnWIlPzwrekmd4Xr0o1lSRZ/kceD3wAvA81W1NsmRwLXAMcDjwGlV9ezSuilJ+2fAL24cFf0Hq2pNVa1t9i8AtlTVHLCl2ZekiTDkB5vE1M2pwFXN9lXAJyfwGpKklpYa9AXcmuTeJBubthVVtbvZfgpYsdADk2xMsjXJ1iX2QdIyZTXfzlKvunl/Ve1K8mbgtiQ/7z9YVZWkFnpgVW0CNgHs7xxJ2h9Dvr0lBX1V7Wru9yT5HnAC8HSSlVW1O8lKYM8Y+ilJgAE/ipGnbpK8Nsnr57eB9cA24CbgrOa0s4Abl9pJSdLollLRrwC+l2T+ef6zqn6Q5B7guiTnAE8Apy29m9J0fGbd+pftX3/nrVPqicD16Ec1ctBX1S+Bdy3Q/gzw4aV0SpKGYcAvziUQpP34zLr1bL7j5pcaHruMDWf3Nq3sDx6G/GAGvdTWcX/D5m/1Ng18dYlBLy1g37n5hXz95P+idw2CgT8tVvPtuKiZpE4y5Nsz6KV9fGbdejZ/a3BFr+nYvt2QH5ZTN9J+XH3xM2z4wCcAel/KPnYZG87uTdH0pm1euof1Tt/ooGXQS435efnN31rPi49vBnphD+wN/Pn9p+6aQgfVaj36HTsOTF+6xKCX1Hkui7A4g15qYb6S18HHkB/MoJcWcOZX3/iy/X2D/vwfvKVvfl46uBn00j42nH0rm++4mcd+/cDetjM/fRFXX/zM3j8A13Irp/9gvWE/ZVbz7Xh5paROMuTbs6KX9jG/vs1xR71zb9s/3PCPnPnpi7iW3iWUp7N+b1UP/jL2QDLgh2dFL7XQH/qaHpcpHo0VvdSYr8o3fOATbL7j5r3XzgMvX8VS6hiDXtrH9Xfeujfs1Q1W8otz6kZSpxnyg1nRSwuYr+p1cDPk2zHopf3Y90qafdeoP92FzNQRBr3U0vV33srp67yc8mBhNd+eQS+pUwz44Rn00hCs5KfLZYpH41U3kjTjrOgldZ7LIixuYEWf5Moke5Js62s7MsltSbY390c07UlyWZIdSR5I8p5Jdl6SDPnB2kzdfBs4eZ+2C4AtVTUHbGn2AT4OzDW3jcDl4+mmJL2SId/OwKCvqjuA3+zTfCpwVbN9FfDJvvbvVM+dwOFJVo6rs5Kk4Y36ZeyKqtrdbD8FrGi2VwFP9p23s2l7hSQbk2xNsnXEPkhaxqzm21vyl7FVVUlqhMdtAjYBjPJ4ScuTAT+8USv6p+enZJr7PU37LmB133lHN22StGSuRz+aUYP+JuCsZvss4Ma+9i80V9+sA57rm+KRJE3BwKmbJJuBk4CjkuwELga+BlyX5BzgCeC05vRbgFOAHcAfgLMn0GdJehkr+cUNDPqq2rCfQx9e4NwCzl1qpySpLUN+MJdAkNRZhnw7Br2kTjLk2zPoJWnGuaiZpE6xkh+eFb2kzmizHr1eyYpeUucZ8IuzopekGWfQS+o0q/nBDHpJnWXIt2PQS+okQ749g16SZpxX3UjqFCv54VnRS+oM16MfjRW9pM4z4BdnRS9JM86gl9RpVvODGfSSOsuQb8egl9RJhnx7Br0kzTivupHUKVbyw7Oil9QZrkc/Git6SZ1nwC/Oil5Spxnygw0M+iRXJtmTZFtf21eS7Epyf3M7pe/YhUl2JHk0yccm1XFJUjttKvpvAycv0H5pVa1pbrcAJDkeOAN4R/OYf09yyLg6K0n9rObbGRj0VXUH8JuWz3cqcE1V/bGqfgXsAE5YQv8kaUGGfHtLmaP/UpIHmqmdI5q2VcCTfefsbNokaSzm5gz5YY0a9JcDxwFrgN3A14d9giQbk2xNsnXEPkiSWhgp6Kvq6ap6oapeBL7JS9Mzu4DVface3bQt9BybqmptVa0dpQ+Slh/Xox/NSEGfZGXf7qeA+StybgLOSHJokmOBOeDupXVRkha3fbshv5iBP5hKshk4CTgqyU7gYuCkJGuAAh4HvghQVQ8luQ54GHgeOLeqXphM1yXJgG9jYNBX1YYFmq9Y5PxLgEuW0ilJ0vj4y1hJnWU1345BL6mTDPn2XNRMUqcY8MOzopekGWfQS+oM16MfjVM3kjrPgF+cFb2kTjPkBzPoJWnGGfSSOstqvh2DXlInGfLt+WWspE4x4IdnRS+pM1ymeDQGvSTNOKduJHWelfzirOgldZohP5hBL6mzDPl2DHpJmnEGvaROsppvzy9jJXWKAT88K3pJneEyxaMx6CVpxjl1I6nzrOQXZ0UvqdMM+cEMekmdZci3MzDok6xOcnuSh5M8lOS8pv3IJLcl2d7cH9G0J8llSXYkeSDJeyb9JiRJ+9emon8eOL+qjgfWAecmOR64ANhSVXPAlmYf4OPAXHPbCFw+9l5LWvas5tsbGPRVtbuq7mu2fw88AqwCTgWuak67Cvhks30q8J3quRM4PMnKsfdc0rI0N2fID2uoOfokxwDvBu4CVlTV7ubQU8CKZnsV8GTfw3Y2bZK0JK5HP5rWl1cmeR1wPfDlqvpdkr3HqqqS1DAvnGQjvakdSdIEtQr6JK+mF/JXV9UNTfPTSVZW1e5mamZP074LWN338KObtpepqk3Apub5h/ojIUn9rOQX1+aqmwBXAI9U1Tf6Dt0EnNVsnwXc2Nf+hebqm3XAc31TPJI0Vob8YG0q+hOBzwMPJrm/absI+BpwXZJzgCeA05pjtwCnADuAPwBnj7XHktQw5NsZGPRV9RMg+zn84QXOL+DcJfZLkhZlyLfnL2Mlaca5qJmkTrGSH54VvaTOcD360VjRS+o8A35xVvSSNOMMekmdZjU/mEEvqbMM+XYMekmdZMi3Z9BL0ozzqhtJnWIlPzwrekmd4Xr0o7Gil9R5BvzirOglacYZ9JI6zWp+MINeUmcZ8u0Y9JI6yZBvz6CXpBnnVTeSOsVKfnhW9JI6w/XoR2NFL6nzDPjFWdFL6jRDfjCDXpJmnEEvqbOs5tsx6CV1kiHf3sCgT7I6ye1JHk7yUJLzmvavJNmV5P7mdkrfYy5MsiPJo0k+Nsk3IGl5mZsz5IfV5qqb54Hzq+q+JK8H7k1yW3Ps0qr65/6TkxwPnAG8A3gL8KMkf1lVL4yz45KkdgZW9FW1u6rua7Z/DzwCrFrkIacC11TVH6vqV8AO4IRxdFbS8uZ69KMZao4+yTHAu4G7mqYvJXkgyZVJjmjaVgFP9j1sJwv8YUiyMcnWJFuH7rUk9dm+3ZBfTOugT/I64Hrgy1X1O+By4DhgDbAb+PowL1xVm6pqbVWtHeZxktTPgB+sVdAneTW9kL+6qm4AqKqnq+qFqnoR+CYvTc/sAlb3Pfzopk2SNAVtrroJcAXwSFV9o699Zd9pnwK2Nds3AWckOTTJscAccPf4uixJPVbz7bS56uZE4PPAg0nub9ouAjYkWQMU8DjwRYCqeijJdcDD9K7YOdcrbiSNmyHf3sCgr6qfAFng0C2LPOYS4JIl9EuSFmTAD89fxkrSjDPoJXWG69GPxvXoJXWeAb84K3pJnWbID2bQS9KMM+gldZbVfDsGvaROMuTb88tYSZ1iwA/Pil5SZ7hM8WgMekmacU7dSOo8K/nFWdFL6jRDfjCDXlJnGfLtGPSSNOMMekmdZDXfnl/GSuoUA354VvSSOsNlikdj0EvSjHPqRlLnWckvLlU17T6Q5L+B/wF+Pe2+TNlROAaOQY/j4BjA4DH4i6p606AnOSiCHiDJ1qpaO+1+TJNj4BjMcxwcAxjfGDhHL0kzzqCXpBl3MAX9pml34CDgGDgG8xwHxwDGNAYHzRy9JGkyDqaKXpI0AVMP+iQnJ3k0yY4kF0y7P5OU5Moke5Js62s7MsltSbY390c07UlyWTMuDyR5z/R6Pj5JVie5PcnDSR5Kcl7TvmzGIclhSe5O8rNmDL7atB+b5K7mvV6b5DVN+6HN/o7m+DHT7P84JTkkyU+T3NzsL8cxeDzJg0nuT7K1aRvr52GqQZ/kEODfgI8DxwMbkhw/zT5N2LeBk/dpuwDYUlVzwJZmH3pjMtfcNgKXH6A+TtrzwPlVdTywDji3+TdfTuPwR+BDVfUuYA1wcpJ1wD8Bl1bV24FngXOa888Bnm3aL23OmxXnAY/07S/HMQD4YFWt6buUcryfh6qa2g14H/DDvv0LgQun2acD8J6PAbb17T8KrGy2VwKPNtv/AWxY6LxZugE3Ah9druMA/BlwH/Beej+MeVXTvvezAfwQeF+z/armvEy772N470c3IfYh4GYgy20MmvfzOHDUPm1j/TxMe+pmFfBk3/7Opm05WVFVu5vtp4AVzfbMj03z3+93A3exzMahmbK4H9gD3AY8Bvy2qp5vTul/n3vHoDn+HPDGA9vjifgX4G+BF5v9N7L8xgCggFuT3JtkY9M21s+Da90cRKqqkiyLy6CSvA64HvhyVf0uyd5jy2EcquoFYE2Sw4HvAX815S4dUEk+AeypqnuTnDTt/kzZ+6tqV5I3A7cl+Xn/wXF8HqZd0e8CVvftH920LSdPJ1kJ0NzvadpndmySvJpeyF9dVTc0zctuHACq6rfA7fSmKQ5PMl989b/PvWPQHP9z4JkD3NVxOxH46ySPA9fQm775V5bXGABQVbua+z30/uifwJg/D9MO+nuAueab9tcAZwA3TblPB9pNwFnN9ln05qzn27/QfMu+Dniu779ynZVe6X4F8EhVfaPv0LIZhyRvaip5kvwpve8oHqEX+J9tTtt3DObH5rPAj6uZoO2qqrqwqo6uqmPofe5/XFVnsozGACDJa5O8fn4bWA9sY9yfh4Pgi4hTgF/Qm6P8u2n3Z8LvdTOwG/g/enNr59CbZ9wCbAd+BBzZnBt6VyQ9BjwIrJ12/8c0Bu+nNyf5AHB/cztlOY0D8E7gp80YbAP+vml/G3A3sAP4LnBo035Ys7+jOf62ab+HMY/HScDNy3EMmvf7s+b20HwGjvvz4C9jJWnGTXvqRpI0YQa9JM04g16SZpxBL0kzzqCXpBln0EvSjDPoJWnGGfSSNOP+H3YpRPiKyEtSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f685de026d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6858d62f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import package needed\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\" # make window not appear\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "from myUtil import make_anim\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False) # environment interface to game\n",
    "env.reset_game()\n",
    "env.act(0) # dummy input to get screen correct\n",
    "\n",
    "# get rgb screen\n",
    "screen = env.getScreenRGB()\n",
    "plt.imshow(screen)\n",
    "print(screen.shape)\n",
    "\n",
    "# get grayscale screen\n",
    "plt.figure()\n",
    "screen = env.getScreenGrayscale()\n",
    "plt.imshow(screen, cmap='gray')\n",
    "print(screen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input size\n",
    "screen_width = 80\n",
    "screen_height = 80\n",
    "num_stack = 4\n",
    "def preprocess(screen):\n",
    "    #screen = skimage.color.rgb2gray(screen)\n",
    "    screen = skimage.transform.resize(screen, [screen_width, screen_height])\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     23,
     120,
     135,
     160,
     164
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "MIN_EXPLORING_RATE = 10e-4\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, num_action, t=0, discount_factor=0.99):\n",
    "        self.exploring_rate = 0.1\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "        self.name = name\n",
    "        with tf.variable_scope(name):\n",
    "            self.build_model()\n",
    "            \n",
    "        self.ckpt_path = './checkpoints/dualing_DQN'\n",
    "        if not os.path.exists(self.ckpt_path):\n",
    "            os.makedirs(self.ckpt_path)\n",
    "\n",
    "        self.vars_to_save = [\n",
    "            var for var in tf.trainable_variables() if name in var.name\n",
    "        ]\n",
    "        self.saver = tf.train.Saver(var_list=self.vars_to_save, max_to_keep=30)\n",
    "    def build_model(self):\n",
    "        # input: current screen, selected action and reward\n",
    "        self.input_screen = tf.placeholder(\n",
    "            tf.float32, shape=[None, screen_width, screen_height, num_stack])\n",
    "        self.action = tf.placeholder(tf.int32, [None])\n",
    "        self.reward = tf.placeholder(tf.float32, [None])\n",
    "        self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "\n",
    "        def net(screen, reuse=False):\n",
    "            with tf.variable_scope(\n",
    "                    \"layers\",\n",
    "                    reuse=reuse,\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=1e-2)):\n",
    "                conv1 = tf.layers.conv2d(\n",
    "                    inputs=screen,\n",
    "                    filters=32,\n",
    "                    kernel_size=[8, 8],\n",
    "                    strides=[4, 4],\n",
    "                    padding='SAME',\n",
    "                    activation=tf.nn.relu)\n",
    "                pool1 = tf.layers.max_pooling2d(\n",
    "                    conv1, pool_size=[2, 2], strides=[2, 2], padding='SAME')\n",
    "\n",
    "                conv2 = tf.layers.conv2d(\n",
    "                    inputs=pool1,\n",
    "                    filters=64,\n",
    "                    kernel_size=[4, 4],\n",
    "                    strides=[2, 2],\n",
    "                    padding='SAME',\n",
    "                    activation=tf.nn.relu)\n",
    "                conv3 = tf.layers.conv2d(\n",
    "                    inputs=conv2,\n",
    "                    filters=64,\n",
    "                    kernel_size=[3, 3],\n",
    "                    strides=[1, 1],\n",
    "                    padding='SAME',\n",
    "                    activation=tf.nn.relu)\n",
    "                flat = tf.contrib.layers.flatten(conv3)\n",
    "                \n",
    "                # advantage estimation\n",
    "                advantage_net = tf.layers.dense(\n",
    "                    inputs=flat, units=512, activation=tf.nn.relu\n",
    "                )\n",
    "                advantage_estimate = tf.layers.dense(\n",
    "                    inputs=advantage_net,\n",
    "                    units=self.num_action\n",
    "                )\n",
    "                \n",
    "                # V estimation\n",
    "                V_net = tf.layers.dense(\n",
    "                    inputs=flat, units=512, activation=tf.nn.relu\n",
    "                )\n",
    "                V_estimate = tf.layers.dense(\n",
    "                    inputs=V_net,\n",
    "                    units=1\n",
    "                )\n",
    "                \n",
    "                # combine V(s) and advantage(s, a) to get Q(s, a)\n",
    "                Q = V_estimate + tf.subtract(\n",
    "                    advantage_estimate,\n",
    "                    tf.reduce_mean(\n",
    "                        advantage_estimate,\n",
    "                        axis=1,\n",
    "                        keep_dims=True\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                return Q\n",
    "\n",
    "        # optimize\n",
    "        self.output = net(\n",
    "            self.input_screen\n",
    "        )  # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "        index = tf.stack(\n",
    "            [tf.range(tf.shape(self.action)[0]), self.action], axis=1)\n",
    "        self.esti_Q = tf.gather_nd(\n",
    "            self.output,\n",
    "            index)  # Q(s,a,theta) for selected action, shape (batch_size, 1)\n",
    "\n",
    "        self.max_Q = tf.reduce_max(\n",
    "            self.output, axis=1)  # max(Q(s',a',theta')), shape (batch_size, 1)\n",
    "        self.tar_Q = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # loss = E[r+max(Q(s',a',theta'))-Q(s,a,theta)]\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.square(self.reward + self.discount_factor * self.tar_Q -\n",
    "                      self.esti_Q))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    "        self.g_gvs = optimizer.compute_gradients(\n",
    "            self.loss,\n",
    "            var_list=[v for v in tf.global_variables() if self.name in v.name])\n",
    "        self.train_op = optimizer.apply_gradients(self.g_gvs)\n",
    "        self.pred = tf.argmax(\n",
    "            self.output, axis=1\n",
    "        )  # select action with highest action-value, only used in inference\n",
    "\n",
    "    def select_action(self, input_screen, sess):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(num_action)  # Select a random action\n",
    "        else:\n",
    "            input_screen = np.array(input_screen).transpose([1, 2, 0])\n",
    "            feed_dict = {\n",
    "                self.input_screen: input_screen[None, :],\n",
    "                self.is_training: False,\n",
    "            }\n",
    "            action = sess.run(\n",
    "                self.pred,\n",
    "                feed_dict=feed_dict)[0]  # Select the action with the highest q\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, input_screens, actions, rewards,\n",
    "                      input_screens_plum, terminal, target_netwrok):\n",
    "        # use max_Q estimate from target one to update online one\n",
    "        feed_dict = {\n",
    "            target_netwrok.input_screen:\n",
    "            np.array(input_screens_plum).transpose([0, 2, 3, 1]),\n",
    "            target_netwrok.is_training:\n",
    "            True,\n",
    "        }\n",
    "        max_Q = sess.run(target_netwrok.max_Q, feed_dict=feed_dict)\n",
    "        max_Q *= ~np.array(terminal)\n",
    "        feed_dict = {\n",
    "            self.input_screen: np.array(input_screens).transpose([0, 2, 3, 1]),\n",
    "            self.tar_Q: max_Q,\n",
    "            self.action: actions,\n",
    "            self.reward: rewards,\n",
    "            self.is_training: True,\n",
    "        }\n",
    "        loss, _ = sess.run([self.loss, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def update_parameters(self, episode):\n",
    "        if self.exploring_rate > MIN_EXPLORING_RATE:\n",
    "            self.exploring_rate -= (0.1 - MIN_EXPLORING_RATE) / 3000000\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0\n",
    "        \n",
    "    def preprocess(self, screen):\n",
    "        #screen = skimage.color.rgb2gray(screen)\n",
    "        screen = skimage.transform.resize(screen, [screen_width, screen_height])\n",
    "        return screen\n",
    "    \n",
    "    def _get_ckpt_name(self, episode):\n",
    "        return os.path.join(self.ckpt_path, '%s_epidode_%d' % (self.name, episode))\n",
    "    \n",
    "    def save_ckpt(self, sess, episode):\n",
    "        print('Saving variables of %s agent(%d episode)...' % (self.name, episode), end='')\n",
    "        self.saver.save(sess, self._get_ckpt_name(episode))\n",
    "        print('Done.')\n",
    "    \n",
    "    def load_ckpt(self, sess, episode):\n",
    "        print('Loading variables of %s agent(%d episode)...' % (self.name, episode), end='')\n",
    "        self.saver.restore(sess, self._get_ckpt_name(episode))\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def get_update_ops():\n",
    "    # return operations assign weight to target network\n",
    "    src_vars = [v for v in tf.global_variables() if 'online' in v.name]\n",
    "    tar_vars = [v for v in tf.global_variables() if 'target' in v.name]\n",
    "    update_ops = []\n",
    "    for src_var, tar_var in zip(src_vars, tar_vars):\n",
    "        update_ops.append(tar_var.assign(src_var))\n",
    "    return update_ops\n",
    "\n",
    "\n",
    "def update_target(update_ops, sess):\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init agent\n",
    "tf.reset_default_graph()\n",
    "num_action = len(env.getActionSet())\n",
    "\n",
    "# agent for frequently updating\n",
    "online_agent = Agent('online', num_action)\n",
    "\n",
    "# agent for slow updating\n",
    "target_agent = Agent('target', num_action)\n",
    "update_ops = get_update_ops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1,
     5,
     10
    ]
   },
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.experiences = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.experiences) >= self.buffer_size:\n",
    "            self.experiences.pop(0)\n",
    "        self.experiences.append(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"\n",
    "        sameple experience from buffer\n",
    "        \"\"\"\n",
    "        if size > len(self.experiences):\n",
    "            experiences_idx = np.random.choice(\n",
    "                len(self.experiences), size=size)\n",
    "        else:\n",
    "            experiences_idx = np.random.choice(\n",
    "                len(self.experiences), size=size, replace=False)\n",
    "        # from all sampled experiences, extract a tuple of (s,a,r,s')\n",
    "        screens = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        screens_plum = []\n",
    "        terminal = []\n",
    "        for i in range(size):\n",
    "            screens.append(self.experiences[experiences_idx[i]][0])\n",
    "            actions.append(self.experiences[experiences_idx[i]][1])\n",
    "            rewards.append(self.experiences[experiences_idx[i]][2])\n",
    "            screens_plum.append(self.experiences[experiences_idx[i]][3])\n",
    "            terminal.append(self.experiences[experiences_idx[i]][4])\n",
    "        return screens, actions, rewards, screens_plum, terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init all\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variables of online agent(600 episode)...INFO:tensorflow:Restoring parameters from ./checkpoints/dualing_DQN/online_epidode_600\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liu/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best t so far: 61\n",
      "[600] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999996700000001, loss: 0.009607061743736267\n",
      "[MoviePy] >>>> Building video movie/DQN-600.webm\n",
      "[MoviePy] Writing video movie/DQN-600.webm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 57.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: movie/DQN-600.webm \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving variables of online agent(600 episode)...Done.\n",
      "[610] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999963700000004, loss: 0.0032805860973894596\n",
      "[620] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999930700000008, loss: 0.0003085833159275353\n",
      "[630] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999897700000011, loss: 0.02977578528225422\n",
      "[640] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999864700000015, loss: 0.0002856984210666269\n",
      "[650] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999831700000018, loss: 0.02808445878326893\n",
      "[660] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999798700000022, loss: 0.00019146938575431705\n",
      "[670] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999765700000025, loss: 0.00017615324759390205\n",
      "[680] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999732700000029, loss: 0.00018444389570504427\n",
      "[690] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999699700000032, loss: 9.243444947060198e-05\n",
      "[700] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999666700000036, loss: 0.00045465206494554877\n",
      "[710] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.0999963370000004, loss: 0.0008769859559834003\n",
      "[720] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999600700000043, loss: 0.0002974361996166408\n",
      "Saving variables of online agent(720 episode)...Done.\n",
      "[730] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999567700000046, loss: 0.0011813007295131683\n",
      "[740] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.0999953470000005, loss: 0.0005035549402236938\n",
      "[750] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999501700000053, loss: 0.024206796661019325\n",
      "[760] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.09999468700000057, loss: 0.0010315600084140897\n",
      "[770] time live:61, cumulated reward: 5.099999999999994, exploring rate: 0.0999943570000006, loss: 0.0005123076261952519\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "update_every_t_step = 6\n",
    "print_every_episode = 10\n",
    "save_video_every_episode = 200\n",
    "ckpt_every = 400\n",
    "NUM_EPISODE = 100000\n",
    "NUM_EXPLORE = 1000\n",
    "\n",
    "# we can redefine origin reward function\n",
    "reward_values = {\n",
    "    \"positive\": 1,  # reward pass a pipe\n",
    "    \"tick\": 0.1,  # reward per timestamp\n",
    "    \"loss\": -1,  # reward of gameover\n",
    "}\n",
    "\n",
    "# init buffer\n",
    "buffer = Replay_buffer()\n",
    "game = FlappyBird()\n",
    "env = PLE(\n",
    "    game,\n",
    "    fps=30,\n",
    "    display_screen=False,\n",
    "    reward_values=reward_values,\n",
    "    rng=np.random.RandomState(1))\n",
    "\n",
    "restore_epoch = 600\n",
    "if restore_epoch > 0:\n",
    "    online_agent.load_ckpt(sess, restore_epoch)\n",
    "    update_target(update_ops, sess)\n",
    "\n",
    "best_t = 0\n",
    "for episode in range(restore_epoch, restore_epoch + NUM_EPISODE + 1):\n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "    env.act(0)  # dummy input to make sure input screen is correct\n",
    "\n",
    "    # record frame\n",
    "    if episode % save_video_every_episode == 0:\n",
    "        frames = [env.getScreenRGB()]\n",
    "\n",
    "    # for every 500 episodes, shutdown exploration to see performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        online_agent.shutdown_explore()\n",
    "\n",
    "    # grayscale input screen for this episode\n",
    "    input_screens = [preprocess(env.getScreenGrayscale())] * 4\n",
    "\n",
    "    # experience for this episode, store all (s,a,r,s') tuple\n",
    "    experience = []\n",
    "\n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0\n",
    "\n",
    "    t = 0\n",
    "    while not env.game_over():\n",
    "\n",
    "        # feed four previous screen, select an action\n",
    "        action = online_agent.select_action(input_screens[-4:], sess)\n",
    "\n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "\n",
    "        # record frame\n",
    "        if episode % save_video_every_episode == 0:\n",
    "            frames.append(env.getScreenRGB())\n",
    "\n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "\n",
    "        # append grayscale screen for this episode\n",
    "        input_screens.append(preprocess(env.getScreenGrayscale()))\n",
    "\n",
    "        # append experience for this episode\n",
    "        buffer.add((input_screens[-5:-1], action, reward, input_screens[-4:],\n",
    "                    env.game_over()))\n",
    "        t += 1\n",
    "\n",
    "        # update agent\n",
    "    if t > best_t:\n",
    "        best_t = t\n",
    "        print(\"Best t so far: %d\" % t)\n",
    "        \n",
    "    if episode > NUM_EXPLORE:\n",
    "        train_screens, train_actions,\\\n",
    "        train_rewards, train_screens_plum, terminal = buffer.sample(32)\n",
    "        loss = online_agent.update_policy(train_screens, train_actions,\n",
    "                                          train_rewards, train_screens_plum,\n",
    "                                          terminal, target_agent)\n",
    "    if t % update_every_t_step == 0 and episode > NUM_EXPLORE:\n",
    "        update_target(update_ops, sess)\n",
    "\n",
    "    # update explore rating and learning rate\n",
    "    online_agent.update_parameters(episode)\n",
    "    target_agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0 and episode > NUM_EXPLORE:\n",
    "        print(\n",
    "            \"[{}] time live:{}, cumulated reward: {}, exploring rate: {}, loss: {}\".\n",
    "            format(episode, t, cum_reward, target_agent.exploring_rate, loss))\n",
    "\n",
    "    if episode % save_video_every_episode == 0:  # for every 100 episode, record an animation\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie/DQN-{}.webm\".format(episode), fps=60)\n",
    "    \n",
    "    if episode % ckpt_every == 0 and episode > NUM_EXPLORE:\n",
    "        online_agent.save_ckpt(sess, episode)\n",
    "\n",
    "print(type(input_screens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
